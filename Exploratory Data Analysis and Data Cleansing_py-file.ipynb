{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5196: Data Wranling \n",
    "## Ayush Sharma\n",
    "### 30823293"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction:\n",
    "In this assignment we are provided with three different csv files that are dirty_data, missing_data and outlier_data. For each file we need to perform Exploratory data analysis and do different tasks like in dirty data we have to clean the data and find out the correct value for the incorrect value using python. making date in correct format or finding out correct order price or total order amount. or checking if longitude and latitude is correct etc. Whereas in the missing_data file we have to impute values for all the missing values in data set wherever there is null. for the outlier_file we need to find out the outliers in the data set and remove the outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Library:\n",
    "Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from math import sin, cos, sqrt, atan2, radians\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Reading CSV File of Dirty Data, Missing Data and Outlier Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading Dirty_data csv file into a pandas dataframe\n",
    "dirty_data=pd.read_csv('30823293_dirty_data.csv')\n",
    "missing_data=pd.read_csv('30823293_missing_data.csv')\n",
    "outlier_data=pd.read_csv('30823293_outlier_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describing content of data and Exploring it\n",
    "\n",
    "As we can see in the very first entry below the format of date was supposed to be YYYY-MM-DD but it is YYYY-DD-MM so we will reformat the date and change it to YYYY-MM-DD and look for cases that also need to be reformated. After that in the df.tail comand we can see that 494 is different and the latitude and longitude are interchanged so we will change all the columns if they have such mistake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying top 10 in dirty data\n",
    "dirty_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying last 10 in dirty data\n",
    "dirty_data.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.) Checking All the Columns\n",
    "\n",
    "- Checking the warehouse names and changing the errors\n",
    "- checking the dates format and changing it into correct format i.e. YYYY-MM-DD\n",
    "- Checking if all seasons are corresponding to the correct month\n",
    "- finding the correct distance from warehouse and the correct warehouse\n",
    "- finding if all the customer are happy or not\n",
    "- findinng the correct order price and order total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next 2 cells we have checked the nearest warehouse column and replaced the column as some of the values are repeated in these column but in actual they are same and there are only 3 warehouses i.e Thompson, Nickolson, Bakers as they represent the same warehouse but they are stored in different ways i.e capitalised and lower case so we will convert all the lowe cases to capitalised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Producing the distinct type of values and there count\n",
    "dirty_data.nearest_warehouse.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making all the values capitalised\n",
    "dirty_data.nearest_warehouse=dirty_data.nearest_warehouse.str.capitalize()\n",
    "dirty_data.nearest_warehouse.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Date Re-formating:\n",
    "Earlier I mentioned that there are some dates in YYYY-DD-MM or MM-DD-YYYY format so we will retify the error and reformat the dates in the correct order of YYYY-MM-DD so I have created seperate columns for date month and year and check all the values in they do not belong to that column we will interchange between them like if a month has 2019 or 28 so we will check it and swap it to the correct column and then merge after rectifying the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the date column and making day month and year column in dataframe\n",
    "for i,j in dirty_data.iterrows():\n",
    "    x= dirty_data.loc[i]['date'].split(\"-\")\n",
    "    dirty_data.loc[i,'year']=x[0]\n",
    "    dirty_data.loc[i,'month']=x[1]\n",
    "    dirty_data.loc[i,'day']=x[2]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# itterating over each row and checking year,month, day column if they are in wrong position so interchanging them.\n",
    "for i, j in dirty_data.iterrows():\n",
    "    if int(j['day'])>31 :\n",
    "        dirty_data.loc[i,['year','day']]=dirty_data.loc[i,['day','year']].values\n",
    "        \n",
    "    if int(j['month'])>31:\n",
    "        dirty_data.loc[i,['year','month']]=dirty_data.loc[i,['month','year']].values\n",
    "        \n",
    "    if int(j['month'])>12:\n",
    "        dirty_data.loc[i,['day','month']]=dirty_data.loc[i,['month','day']].values\n",
    "dirty_data['date']=dirty_data['year'].astype(str)+'-'+\\\n",
    "                    dirty_data['month'].astype(str)+'-'+dirty_data['day'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Checking if there is no wrong value in the year month and date column </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking all values in year\n",
    "dirty_data['year'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking all values in day\n",
    "dirty_data['day'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking all values in month\n",
    "dirty_data['month'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing Seasons\n",
    "- After analysing the warehouse column in the dirty data we had a simple issue in the season column. We can clearly See that some rows in season column are capitalised while some are in in lower case so we will keep all the values in capitalised form so we will replace that column.\n",
    "\n",
    "- Another problem we encounter in season column was that some dates where in wrong format so we will check if that season is wrong for that date we change those seasons and set them to actual season in Australia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at each season\n",
    "dirty_data.season.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# capitalising and printing all the values for season\n",
    "dirty_data.season=dirty_data.season.str.capitalize()\n",
    "dirty_data.season.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Seasons printed below are those entries that have their seasons updated as wrong so we will replace these season with the correct seasons which we have done in the cell after the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking all the wrong season based on the month column\n",
    "for i, j in dirty_data.iterrows():\n",
    "    if (dirty_data.loc[i]['month']=='12' or dirty_data.loc[i]['month']=='01' \\\n",
    "        or dirty_data.loc[i]['month']=='02') and dirty_data.loc[i]['season']!='Summer':\n",
    "        print(i, dirty_data.loc[i]['season'])\n",
    "        \n",
    "    if (dirty_data.loc[i]['month']=='03' or dirty_data.loc[i]['month']=='04' \\\n",
    "        or dirty_data.loc[i]['month']=='05') and dirty_data.loc[i]['season']!='Autumn':\n",
    "        print(i, dirty_data.loc[i]['season'])\n",
    "        \n",
    "    if (dirty_data.loc[i]['month']=='06' or dirty_data.loc[i]['month']=='07' \\\n",
    "        or dirty_data.loc[i]['month']=='08') and dirty_data.loc[i]['season']!='Winter':\n",
    "        print(i, dirty_data.loc[i]['season'])\n",
    "    \n",
    "    if (dirty_data.loc[i]['month']=='09' or dirty_data.loc[i]['month']=='10' \\\n",
    "        or dirty_data.loc[i]['month']=='11') and dirty_data.loc[i]['season']!='Spring':\n",
    "        print(i, dirty_data.loc[i]['season'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing all the above values that has wrong season\n",
    "for i, j in dirty_data.iterrows():\n",
    "    if (dirty_data.loc[i]['month']=='12' or dirty_data.loc[i]['month']=='01' or\\\n",
    "        dirty_data.loc[i]['month']=='02') and dirty_data.loc[i]['season']!='Summer':\n",
    "        dirty_data.at[i,'season']='Summer'\n",
    "        \n",
    "    if (dirty_data.loc[i]['month']=='03' or dirty_data.loc[i]['month']=='04' or \\\n",
    "        dirty_data.loc[i]['month']=='05') and dirty_data.loc[i]['season']!='Autumn':\n",
    "        dirty_data.at[i,'season']='Autumn'\n",
    "        \n",
    "    if (dirty_data.loc[i]['month']=='06' or dirty_data.loc[i]['month']=='07' or \\\n",
    "        dirty_data.loc[i]['month']=='08') and dirty_data.loc[i]['season']!='Winter':\n",
    "        dirty_data.at[i,'season']='Winter'\n",
    "    \n",
    "    if ((dirty_data.loc[i]['month']=='09' or dirty_data.loc[i]['month']=='10' or \\\n",
    "         dirty_data.loc[i]['month']=='11') and dirty_data.loc[i]['season']!='Spring'):\n",
    "        dirty_data.at[i,'season']='Spring'\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>checking if all wrong season columns are replaced</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final check for all the seasons replaced if wrong\n",
    "for i, j in dirty_data.iterrows():\n",
    "    if (dirty_data.loc[i]['month']=='12' or dirty_data.loc[i]['month']=='01' \\\n",
    "        or dirty_data.loc[i]['month']=='02') and dirty_data.loc[i]['season']!='Summer':\n",
    "        print(i, dirty_data.loc[i]['season'])\n",
    "        \n",
    "    if (dirty_data.loc[i]['month']=='03' or dirty_data.loc[i]['month']=='04' \\\n",
    "        or dirty_data.loc[i]['month']=='05') and dirty_data.loc[i]['season']!='Autumn':\n",
    "        print(i, dirty_data.loc[i]['season'])\n",
    "        \n",
    "    if (dirty_data.loc[i]['month']=='06' or dirty_data.loc[i]['month']=='07' \\\n",
    "        or dirty_data.loc[i]['month']=='08') and dirty_data.loc[i]['season']!='Winter':\n",
    "        print(i, dirty_data.loc[i]['season'])\n",
    "    \n",
    "    if (dirty_data.loc[i]['month']=='09' or dirty_data.loc[i]['month']=='10' \\\n",
    "        or dirty_data.loc[i]['month']=='11') and dirty_data.loc[i]['season']!='Spring':\n",
    "        print(i, dirty_data.loc[i]['season'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Longitude latitude interchange\n",
    "\n",
    "- In the beginning we have found out that some longitude and latitude are interchanged so we will interchange these values.\n",
    "- As we can see most of the customer_long starts with `144.9` and most of the custpmer_lat starts with `-37.8` but there are few values that are interchanged so we will interchange such values where there is any customer_lat which is greater that 0 as latitudes are all in -ve form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_data.customer_lat.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_data.customer_long.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interchanging all the values if they are wrong they will be interchanged\n",
    "for i, j in dirty_data.iterrows():\n",
    "    if j['customer_lat']>0:\n",
    "        dirty_data.loc[i,['customer_lat','customer_long']]\\\n",
    "        =dirty_data.loc[i,['customer_long','customer_lat']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we see these column all the values are now replaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_data.customer_lat.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_data.customer_long.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customer review\n",
    "We have encounter an issue that correcponding to some customer reviews the is_happy customer is incorrectly filled so for that we will use a library SentimentIntensityAnalyzer where if a `compound score` is greater than `0.05` we will set the column as True i,e customer is happy otherwise we will set it as false which means the customer is not happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instanciating sentiment analyser\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "# creating new colummn for polarity scores\n",
    "dirty_data['rating'] = dirty_data['latest_customer_review']\\\n",
    "                    .apply(lambda row: analyser.polarity_scores(row))\n",
    "\n",
    "# creating a column that has only compound score for each sentiment analysis\n",
    "dirty_data['compound'] = dirty_data['rating']\\\n",
    "                        .apply(lambda score_dict: score_dict['compound'])\n",
    "# inputing all the correct value for customer happiness \n",
    "#i.e. if happy it is tru else it is false if the value of compound score is less than 0.05\n",
    "\n",
    "dirty_data['is_happy_customer_new'] = dirty_data['compound']\\\n",
    "                        .apply(lambda x: True if x>= 0.05 else False)\n",
    "\n",
    "# creating a new column for that\n",
    "dirty_data[\"is_happy_customer\"]=dirty_data['is_happy_customer_new']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if there is no review it will be set as true i.e. customer is happy\n",
    "rowCount = dirty_data.count()\n",
    "rangeofDf = rowCount.iloc[0]\n",
    "for i in range (0, rangeofDf):\n",
    "    if dirty_data.loc[i, 'latest_customer_review'] == 'None':\n",
    "        dirty_data.loc[i, 'is_happy_customer'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance from the warehouse\n",
    "\n",
    "Now in this part we need to fix the error in the nearest warehouse column by calculating the distance between the customer latitude and customer longitude with warehouse latitude and warehouse longitude and for that we have used `haversine distance formulate` so we will be able to find the distance and the difference between the two wrong entries and for this we have used warehouse csv to get the long and latitude of the warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warehouse=pd.read_csv('warehouses.csv')\n",
    "#calculating distance\n",
    "\n",
    "\n",
    "def distance(x1,y1,warehouse_name):\n",
    "    Radius = 6378.0\n",
    "    x2 = radians(warehouse['lat'][warehouse['names'] == warehouse_name])\n",
    "    y2  =  radians(warehouse['lon'][warehouse['names'] == warehouse_name])\n",
    "    dist_long = y2 - y1 \n",
    "    dist_lat = x2 - x1\n",
    "    mat = sin(dist_lat / 2)**2 + cos(x1) * cos(x2) * sin(dist_long / 2)**2 \n",
    "    mat1 = 2 * atan2(sqrt(mat), sqrt(1 - mat))\n",
    "    distance = Radius * mat1\n",
    "    #for i in distance:\n",
    "    return distance, warehouse_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(dirty_data['order_id'])):\n",
    "    final_list = []\n",
    "    dis_list = []\n",
    "    name_list = []\n",
    "    for j in warehouse['names']:\n",
    "        dis,name = distance(radians(dirty_data.customer_lat[i])\\\n",
    "                            ,radians(dirty_data.customer_long[i]),j)\n",
    "        dis_list.append(dis)\n",
    "        name_list.append(name)\n",
    "    final_list = list(zip(dis_list,name_list))\n",
    "    final_dis,final_name = min(final_list)[0],min(final_list)[1]\n",
    "    dirty_data.at[i,'new_distance_to_nearest_warehouse'] = round(final_dis,4)\n",
    "    dirty_data.at[i,'new_nearest_warehouse'] = final_name\n",
    "    \n",
    "dirty_data.nearest_warehouse=dirty_data.new_nearest_warehouse\n",
    "dirty_data.distance_to_nearest_warehouse=dirty_data.new_distance_to_nearest_warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Order Price and Order total\n",
    "\n",
    "- Another issue that we encountered was that the order price and the order total were different based on the coumn shopping cart so for that what we have done is we have splitted the shopping cart and created a matrix of all the occurance of the product and using `np.linalg` we have calculated correct price for each item in the shopping cart with this we have found out the correct order total and order price.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing_copy = missing_data[]\n",
    "missing_copy = missing_data[missing_data.order_price.notnull()]\n",
    "\n",
    "shopping_list=[]\n",
    "for i, j in missing_copy.iterrows():\n",
    "    lst=missing_copy.loc[i]['shopping_cart']\n",
    "    x=lst.split(\"), (\")\n",
    "    shopping_dict={}\n",
    "    for each in x:\n",
    "        shopping_dict[each.split(\",\")[0].replace(\"(\",\"\").replace(\"[\",\"\")\\\n",
    "            .replace(\"\\'\",\"\")]=int(each.split(\",\")[1].replace(\")\",\"\")\\\n",
    "                                   .replace(\"]\",\"\").replace(\"\\'\",\"\").strip())\n",
    "    shopping_list.append(shopping_dict)\n",
    "    \n",
    "\n",
    "uniqueLst=[]        \n",
    "for each in shopping_list:\n",
    "    for e1 in each:\n",
    "        uniqueLst.append(e1)\n",
    "        \n",
    "\n",
    "listshopping_dict=[] \n",
    "uniqueListFinal=[]\n",
    "\n",
    "\n",
    "for h in uniqueLst:\n",
    "    if(h not in uniqueListFinal):\n",
    "        uniqueListFinal.append(h)\n",
    "    \n",
    "\n",
    "len(uniqueListFinal)\n",
    "for l in shopping_list:\n",
    "    shopping_mapping=[]\n",
    "    for each in uniqueListFinal:    \n",
    "        if(each in(l.keys())):\n",
    "            shopping_mapping.append(l.get(each))\n",
    "        else:\n",
    "            shopping_mapping.append(0)\n",
    "    listshopping_dict.append(shopping_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_list = []\n",
    "for i, j in missing_copy.iterrows():\n",
    "    price_list.append(missing_copy.loc[i, \"order_price\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shopping_array = np.array(listshopping_dict[:10])\n",
    "price_array = np.array(price_list[:10])\n",
    "linear_price = np.linalg.solve(shopping_array, price_array)\n",
    "linear_price=list(linear_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "priceDict={}\n",
    "i=0\n",
    "for each in uniqueListFinal:\n",
    "    priceDict[each]=linear_price[i]\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_data[\"new_order_price\"] = 0\n",
    "i=0\n",
    "for x in dirty_data.shopping_cart:\n",
    "    total_amount=0\n",
    "    for x in eval(x):\n",
    "        if x[0] in priceDict.keys():\n",
    "            total_amount=total_amount+x[1]*priceDict[x[0]]\n",
    "    dirty_data.at[i,\"new_order_price\"]=total_amount\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dirty_data['new_order_total'] = 0\n",
    "dirty_data['new_order_total']=(((100-dirty_data['coupon_discount'])/100)\\\n",
    "                               *dirty_data['new_order_price'])+dirty_data['delivery_charges']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we can see the new order price is different in the 9th and 12th entry in the output of below cell we will replace this value and with the new correct value so that we can make sure we dont replace correct values with the wrong ones so in the cell after the next cell we will replace all such wrong values for both order price and order total.\n",
    "- Also we can see in the 487th entry that the order price and new order price is same but the order total is different so we will replace that too as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_data[[\"order_price\",\"new_order_price\",'coupon_discount'\\\n",
    "            ,'delivery_charges','new_order_total','order_total']].head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_data[[\"order_price\",\"new_order_price\",'coupon_discount'\\\n",
    "            ,'delivery_charges','new_order_total','order_total']].tail(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j in dirty_data.iterrows():\n",
    "    if dirty_data.loc[i]['order_total']==dirty_data.loc[i]['new_order_total'] and \\\n",
    "    dirty_data.loc[i]['order_price']!=dirty_data.loc[i]['new_order_price']:\n",
    "        dirty_data.at[i,'order_price']=dirty_data.loc[i]['new_order_price'] \n",
    "    if dirty_data.loc[i]['order_total']!=dirty_data.loc[i]['new_order_total'] and\\\n",
    "    dirty_data.loc[i]['order_price']==dirty_data.loc[i]['new_order_price']:\n",
    "        dirty_data.at[i,'order_total']=dirty_data.loc[i]['new_order_total']\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below 2 cells we have confirmed that all three of the values mentioned above are replaced and we now have the correct value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_data[[\"order_price\",\"new_order_price\",'coupon_discount','delivery_charges','new_order_total','order_total']].head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_data[[\"order_price\",\"new_order_price\",'coupon_discount','delivery_charges','new_order_total','order_total']].tail(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Missing Data\n",
    "\n",
    "In the part 2 of the assignment we have some missing values in our data so we will impute these values. In the cell below we have find out the number of values that are missing in the data frame where the following has the number of missing data:\n",
    "\n",
    "- nearest_warehouse:                `55`\n",
    "- order_price:                      `15`\n",
    "- delivery_charges:                 `40`\n",
    "- order_total:                      `15`\n",
    "- distance_to_nearest_warehouse:    `31`\n",
    "- is_happy_customer:                `40`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display all column with missing values\n",
    "missing_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest Warehouse:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have created a funtion above in the dirty data part named `distance` and we have called it here to find out the distance and the nearest warehourse where there is a missing value.\n",
    "- In the below 3 cells have calculated the distance from the warehouse for each entry and found out the nearest warehourse to the customer longitude and latiude using haversine distance\n",
    "- And in the end I have replaced all the missing values in the nearest warehouse and distance to warehouse from the new nearest warehouse and new distance to warehouse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating distance\n",
    "\n",
    "for i in range(len(missing_data['order_id'])):\n",
    "    final_list = []\n",
    "    dis_list = []\n",
    "    name_list = []\n",
    "    for j in warehouse['names']:\n",
    "        dis,name = distance(radians(missing_data.customer_lat[i]),radians(missing_data.customer_long[i]),j)\n",
    "        dis_list.append(dis)\n",
    "        name_list.append(name)\n",
    "    final_list = list(zip(dis_list,name_list))\n",
    "    final_dis,final_name = min(final_list)[0],min(final_list)[1]\n",
    "    missing_data.at[i,'new_distance_to_nearest_warehouse'] = round(final_dis,4)\n",
    "    missing_data.at[i,'new_nearest_warehouse'] = final_name\n",
    "    \n",
    "#missing_data.nearest_warehouse=missing_data.new_nearest_warehouse\n",
    "#missing_data.distance_to_nearest_warehouse=missing_data.new_distance_to_nearest_warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing all the null values with new warehouse values\n",
    "for i,j in missing_data.iterrows():\n",
    "    if pd.isnull(j['nearest_warehouse']):\n",
    "        missing_data.loc[i,['nearest_warehouse','new_nearest_warehouse']]=missing_data.loc[i,['new_nearest_warehouse','nearest_warehouse']].values\n",
    "        \n",
    "missing_data.new_nearest_warehouse.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in missing_data.iterrows():\n",
    "    if pd.isnull(j['distance_to_nearest_warehouse']):\n",
    "        missing_data.loc[i,['distance_to_nearest_warehouse','new_distance_to_nearest_warehouse']]=missing_data.loc[i,['new_distance_to_nearest_warehouse','distance_to_nearest_warehouse']].values\n",
    "    \n",
    "    \n",
    "missing_data.distance_to_nearest_warehouse.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Happy Customer review:\n",
    " is_happy_customer is a column that has a true or false value in form of 0 and 1 and some values are missing so we will impute these values and find out the correct value for these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instanciating sentiment analyser\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "# creating new colummn for polarity scores\n",
    "missing_data['rating'] = missing_data['latest_customer_review']\\\n",
    "                    .apply(lambda row: analyser.polarity_scores(row))\n",
    "\n",
    "# creating a column that has only compound score for each sentiment analysis\n",
    "missing_data['compound'] = missing_data['rating']\\\n",
    "                        .apply(lambda score_dict: score_dict['compound'])\n",
    "# inputing all the correct value for customer happiness \n",
    "#i.e. if happy it is tru else it is false if the value of compound score is less than 0.05\n",
    "\n",
    "missing_data['is_happy_customer_new'] = missing_data['compound']\\\n",
    "                        .apply(lambda x: 1.0 if x>= 0.05 else 0.0)\n",
    "\n",
    "# creating a new column for that\n",
    "for i,j in missing_data.iterrows():\n",
    "    if pd.isnull(j['is_happy_customer']):\n",
    "        missing_data.loc[i,['is_happy_customer','is_happy_customer_new']]=missing_data.loc[i,['is_happy_customer_new','is_happy_customer']].values\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if all the values have been imputed as we can see we have imputed all the missing values and we can confirm it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data.is_happy_customer.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing order price and order total\n",
    "\n",
    "- now we will update all the missing order price and order total using the linear equation solver we have used earlier to get order price and order total so we will replace such values using the same way and the price we have found for that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding out the price for each value in dataframe\n",
    "missing_data[\"new_order_price\"] = 0\n",
    "i=0\n",
    "for x in missing_data.shopping_cart:\n",
    "    total_amount=0\n",
    "    for x in eval(x):\n",
    "        if x[0] in priceDict.keys():\n",
    "            total_amount=total_amount+x[1]*priceDict[x[0]]\n",
    "    missing_data.at[i,\"new_order_price\"]=total_amount\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating order total\n",
    "missing_data['new_order_total'] = 0\n",
    "missing_data['new_order_total']=(((100-missing_data['coupon_discount'])/100)\\\n",
    "                               *missing_data['new_order_price'])+missing_data['delivery_charges']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding all the values to missing order price and order total\n",
    "for i,j in missing_data.iterrows():\n",
    "    if pd.isnull(j['order_price']):\n",
    "        missing_data.loc[i,['order_price','new_order_price']]=missing_data.loc[i,['new_order_price','order_price']].values\n",
    "    if pd.isnull(j['order_total']):\n",
    "        missing_data.loc[i,['order_total','new_order_total']]=missing_data.loc[i,['new_order_total','order_total']].values\n",
    "        \n",
    "missing_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Delivery charge\n",
    "\n",
    "from above part we have the missing order total and order price we will find out delivery charges for all missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data['new_delivery_charge'] = 0\n",
    "missing_data['new_delivery_charge']=missing_data['order_total']-(((100-missing_data['coupon_discount'])/100)\\\n",
    "                               *missing_data['new_order_price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in missing_data.iterrows():\n",
    "    if pd.isnull(j['delivery_charges']):\n",
    "        missing_data.loc[i,['delivery_charges','new_delivery_charge']]=missing_data.\\\n",
    "        loc[i,['new_delivery_charge','delivery_charges']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Showing all the columns that we have missing data are removed</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part3 Outlier Data:\n",
    "In the outlier file we have to remove all the outlier in the delivery charges column for that we have used a linear regression model to predict the delivery charge and find out the difference between each of them after that we have remove those values from the data where there is a outlier in the difference between the actual delivery charge and orignal delivery charge. in the plot below we have show that there are outliers in the delivery charge column and for each season there is are outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot for outlier in delivery charge\n",
    "plt.figure(figsize=(15,4))\n",
    "sns.boxplot( x = outlier_data[\"delivery_charges\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot for outlier in delivery charge and season\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.boxplot(y='delivery_charges',x='season',hue='season',data=outlier_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding  a column with numeric value to predict delivery charge\n",
    "datasize=dirty_data.count().iloc[0]\n",
    "for x in range (0, datasize):\n",
    "    if dirty_data.loc[x, 'is_expedited_delivery'] == True:\n",
    "           dirty_data.loc[x, 'new_is_expedited_delivery'] = 1.0\n",
    "    else:\n",
    "            dirty_data.loc[x, 'new_is_expedited_delivery'] = 0.0\n",
    "            \n",
    "    if dirty_data.loc[x, 'is_happy_customer'] == True:\n",
    "            dirty_data.loc[x, 'new_is_happy_customer'] = 1.0\n",
    "    else:\n",
    "            dirty_data.loc[x, 'new_is_happy_customer'] = 0.0\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperating all data based on season\n",
    "summer_df=dirty_data[['is_expedited_delivery','distance_to_nearest_warehouse'\\\n",
    "                      ,'is_happy_customer','delivery_charges']][dirty_data.season=='Summer']\n",
    "autumn_df=dirty_data[['is_expedited_delivery','distance_to_nearest_warehouse'\\\n",
    "                      ,'is_happy_customer','delivery_charges']][dirty_data.season=='Autumn']\n",
    "winter_df=dirty_data[['is_expedited_delivery','distance_to_nearest_warehouse'\\\n",
    "                      ,'is_happy_customer','delivery_charges']][dirty_data.season=='Winter']\n",
    "spring_df=dirty_data[['is_expedited_delivery','distance_to_nearest_warehouse'\\\n",
    "                      ,'is_happy_customer','delivery_charges']][dirty_data.season=='Spring']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting Linear regression model:\n",
    "\n",
    "We have used dirty data to fit in linear regression model on the using is_expedited_delivery, distance_to_nearest_warehouse, is_happy_customer columns and target variable is delivery charges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm=LinearRegression()\n",
    "summer_model=lm.fit(summer_df[['is_expedited_delivery','distance_to_nearest_warehouse'\\\n",
    "                      ,'is_happy_customer']],summer_df.delivery_charges)\n",
    "autumn_model=lm.fit(autumn_df[['is_expedited_delivery','distance_to_nearest_warehouse'\\\n",
    "                      ,'is_happy_customer']],autumn_df.delivery_charges)\n",
    "winter_model=lm.fit(winter_df[['is_expedited_delivery','distance_to_nearest_warehouse'\\\n",
    "                      ,'is_happy_customer']],winter_df.delivery_charges)\n",
    "spring_model=lm.fit(spring_df[['is_expedited_delivery','distance_to_nearest_warehouse'\\\n",
    "                      ,'is_happy_customer']],spring_df.delivery_charges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasize1=outlier_data.count().iloc[0]\n",
    "for x in range (0, datasize1):\n",
    "    if outlier_data.loc[x, 'is_expedited_delivery'] == True:\n",
    "           outlier_data.loc[x, 'new_is_expedited_delivery'] = 1.0\n",
    "    else:\n",
    "            outlier_data.loc[x, 'new_is_expedited_delivery'] = 0.0\n",
    "            \n",
    "    if outlier_data.loc[x, 'is_happy_customer'] == True:\n",
    "            outlier_data.loc[x, 'new_is_happy_customer'] = 1.0\n",
    "    else:\n",
    "            outlier_data.loc[x, 'new_is_happy_customer'] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_summer_df=outlier_data[outlier_data.season=='Summer']\n",
    "outlier_autumn_df=outlier_data[outlier_data.season=='Autumn']\n",
    "outlier_winter_df=outlier_data[outlier_data.season=='Winter']\n",
    "outlier_spring_df=outlier_data[outlier_data.season=='Spring']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting:\n",
    "Now we are predicting the delivery charges for the model we have made and then we will remove all the outlier using this predicted value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting values using linear regression\n",
    "outlier_summer_df['predicted_delivery_charges']= summer_model\\\n",
    ".predict(outlier_summer_df[['is_expedited_delivery','distance_to_nearest_warehouse'\\\n",
    "                      ,'is_happy_customer']])\n",
    "\n",
    "\n",
    "outlier_autumn_df['predicted_delivery_charges'] = autumn_model\\\n",
    ".predict(outlier_autumn_df[['is_expedited_delivery','distance_to_nearest_warehouse'\\\n",
    "                      ,'is_happy_customer']])\n",
    "\n",
    "outlier_winter_df['predicted_delivery_charges']= winter_model\\\n",
    ".predict(outlier_winter_df[['is_expedited_delivery','distance_to_nearest_warehouse'\\\n",
    "                      ,'is_happy_customer']])\n",
    "\n",
    "outlier_spring_df['predicted_delivery_charges'] = spring_model\\\n",
    ".predict(outlier_spring_df[['is_expedited_delivery','distance_to_nearest_warehouse'\\\n",
    "                      ,'is_happy_customer']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing outlier\n",
    "\n",
    "Now we are removing outlier by finding out the IQR and Lower and upper bound and remove all the values that are below and above these values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "outlier_df = pd.concat([outlier_summer_df, outlier_autumn_df, outlier_winter_df, outlier_spring_df])\n",
    "outlier_df\n",
    "outlier_df['delivery_charges_difference'] = outlier_df['delivery_charges'] - outlier_df['predicted_delivery_charges']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "outlier_df_spring= outlier_df[outlier_df.season==\"Spring\"]\n",
    "#print(outlier_df_summer)\n",
    "lst=outlier_df_spring['delivery_charges_difference'].values.tolist()\n",
    "lst.sort()\n",
    "median=statistics.median(lst)\n",
    "lst1=lst[0:int(len(lst)/2)]\n",
    "lst2=lst[int(len(lst)/2):len(lst)]\n",
    "median1=statistics.median( lst1)\n",
    "print(\"median1\")\n",
    "print(median1)\n",
    "median2= statistics.median(lst2)\n",
    "print(\"median2\")\n",
    "print(median2)\n",
    "IQR= median2-median1\n",
    "print('IQR')\n",
    "print(IQR)\n",
    "\n",
    "lower_bond= median1- 1.5*(IQR)\n",
    "upper_bond= median2+ 1.5 *(IQR)\n",
    "print(lower_bond)\n",
    "print(upper_bond)\n",
    "outlier_df_spring_new=outlier_df_spring.copy()\n",
    "for i,j in outlier_df_spring.iterrows():\n",
    "    \n",
    "    if(outlier_df_spring.loc[i,'delivery_charges_difference']<lower_bond):\n",
    "        outlier_df_spring_new.drop(i,inplace=True)\n",
    "    if(outlier_df_spring.loc[i,'delivery_charges_difference']>upper_bond):\n",
    "        outlier_df_spring_new.drop(i,inplace=True)    \n",
    "len(outlier_df_spring_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "outlier_df_summer= outlier_df[outlier_df.season==\"Summer\"]\n",
    "#print(outlier_df_summer)\n",
    "lst=outlier_df_summer['delivery_charges_difference'].values.tolist()\n",
    "lst.sort()\n",
    "median=statistics.median(lst)\n",
    "lst1=lst[0:int(len(lst)/2)]\n",
    "lst2=lst[int(len(lst)/2):len(lst)]\n",
    "median1=statistics.median( lst1)\n",
    "print(\"median1\")\n",
    "print(median1)\n",
    "median2= statistics.median(lst2)\n",
    "print(\"median2\")\n",
    "print(median2)\n",
    "IQR= median2-median1\n",
    "print('IQR')\n",
    "print(IQR)\n",
    "\n",
    "lower_bond= median1- 1.5*(IQR)\n",
    "upper_bond= median2+ 1.5 *(IQR)\n",
    "print(lower_bond)\n",
    "print(upper_bond)\n",
    "outlier_df_summer_new=outlier_df_summer.copy()\n",
    "for i,j in outlier_df_summer.iterrows():\n",
    "    \n",
    "    if(outlier_df_summer.loc[i,'delivery_charges_difference']<lower_bond):\n",
    "        outlier_df_summer_new.drop(i,inplace=True)\n",
    "    if(outlier_df_summer.loc[i,'delivery_charges_difference']>upper_bond):\n",
    "        outlier_df_summer_new.drop(i,inplace=True)    \n",
    "len(outlier_df_summer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "outlier_df_winter= outlier_df[outlier_df.season==\"Winter\"]\n",
    "#print(outlier_df_summer)\n",
    "lst=outlier_df_winter['delivery_charges_difference'].values.tolist()\n",
    "lst.sort()\n",
    "median=statistics.median(lst)\n",
    "lst1=lst[0:int(len(lst)/2)]\n",
    "lst2=lst[int(len(lst)/2):len(lst)]\n",
    "median1=statistics.median( lst1)\n",
    "print(\"median1\")\n",
    "print(median1)\n",
    "median2= statistics.median(lst2)\n",
    "print(\"median2\")\n",
    "print(median2)\n",
    "IQR= median2-median1\n",
    "print('IQR')\n",
    "print(IQR)\n",
    "\n",
    "lower_bond= median1- 1.5*(IQR)\n",
    "upper_bond= median2+ 1.5 *(IQR)\n",
    "print(lower_bond)\n",
    "print(upper_bond)\n",
    "outlier_df_winter_new=outlier_df_winter.copy()\n",
    "for i,j in outlier_df_winter.iterrows():\n",
    "    \n",
    "    if(outlier_df_winter.loc[i,'delivery_charges_difference']<lower_bond):\n",
    "        outlier_df_winter_new.drop(i,inplace=True)\n",
    "    if(outlier_df_winter.loc[i,'delivery_charges_difference']>upper_bond):\n",
    "        outlier_df_winter_new.drop(i,inplace=True)    \n",
    "len(outlier_df_winter)\n",
    "len(outlier_df_winter_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "outlier_df_autumn= outlier_df[outlier_df.season==\"Autumn\"]\n",
    "#print(outlier_df_summer)\n",
    "lst=outlier_df_autumn['delivery_charges_difference'].values.tolist()\n",
    "lst.sort()\n",
    "median=statistics.median(lst)\n",
    "lst1=lst[0:int(len(lst)/2)]\n",
    "lst2=lst[int(len(lst)/2):len(lst)]\n",
    "median1=statistics.median( lst1)\n",
    "print(\"median1\")\n",
    "print(median1)\n",
    "median2= statistics.median(lst2)\n",
    "print(\"median2\")\n",
    "print(median2)\n",
    "IQR= median2-median1\n",
    "print('IQR')\n",
    "print(IQR)\n",
    "\n",
    "lower_bond= median1- 1.5*(IQR)\n",
    "upper_bond= median2+ 1.5 *(IQR)\n",
    "print(lower_bond)\n",
    "print(upper_bond)\n",
    "outlier_df_autumn_new=outlier_df_autumn.copy()\n",
    "for i,j in outlier_df_autumn.iterrows():\n",
    "    \n",
    "    if(outlier_df_autumn.loc[i,'delivery_charges_difference']<lower_bond):\n",
    "        outlier_df_autumn_new.drop(i,inplace=True)\n",
    "    if(outlier_df_autumn.loc[i,'delivery_charges_difference']>upper_bond):\n",
    "        outlier_df_autumn_new.drop(i,inplace=True)    \n",
    "len(outlier_df_autumn)\n",
    "len(outlier_df_autumn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDF=pd.concat([outlier_df_autumn_new,outlier_df_spring_new,outlier_df_summer_new,outlier_df_winter_new])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots to show Outliers are removed\n",
    "\n",
    "As we can see below all the outliers are removed from our final data and there are no outlier point that are showing in both the plots thus we can say now that outliers have been handles and we have removed them now we will check all the files and remove any extra columns \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "sns.boxplot(x='delivery_charges_difference',data=finalDF)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "sns.boxplot(x='delivery_charges_difference',y='season',data=finalDF)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Final Datframe Frame for Writing CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDF.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_data.drop(['year','month','day','rating','compound','is_happy_customer_new'\\\n",
    "                 ,'new_distance_to_nearest_warehouse','new_nearest_warehouse','new_order_price'\\\n",
    "                 ,'new_order_total','new_is_expedited_delivery'\\\n",
    "                 ,'new_is_happy_customer'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data.drop(['new_distance_to_nearest_warehouse','new_nearest_warehouse','rating','compound'\\\n",
    "                   ,'is_happy_customer_new','new_order_price','new_order_total'\\\n",
    "                   ,'new_delivery_charge'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDF.drop(['new_is_expedited_delivery','new_is_happy_customer'\\\n",
    "              ,'predicted_delivery_charges','delivery_charges_difference'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDF.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writting Outlier dataframe into CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_data.to_csv('30823293_dirty_data_solution.csv',index=False)\n",
    "missing_data.to_csv('30823293_missing_data_solution.csv',index=False)\n",
    "finalDF.to_csv('30823293_outlier_data_solution.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion:\n",
    "\n",
    "From this assignment I have learned how to analyze and wrangle data. I have learned how to work with different formats of data like handling of the date column. I also learned how to do sentiment analysis using the given functions and libraries in python. This allowed me to learn how by just a text we can find out the sentiment of the person. I also learned how to handle and calculate distances between two points given their latitude and longitude values. This assignment also allowed me to learn about handling the missing data and the outliers data. I learned to calculate the outlier values and use its concept to remove them accordingly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference:\n",
    "\n",
    "https://stackoverflow.com/questions/26886653/pandas-create-new-column-based-on-values-from-other-columns-apply-a-function-o\n",
    "https://stackoverflow.com/questions/19412462/getting-distance-between-two-points-based-on-latitude-longitude"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
